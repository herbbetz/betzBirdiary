For Raspicam capture the frames per second are most using compiled code like 'raspivid/libcamera-vid' or 'OpenCV C++ lib' (Rust not yet evaluated), followed by python:
"While Picamera2 is primarily a Python library, it can utilize hardware acceleration through MJPEG encoding." (perplexity 11/24)

python3-libcamera (import libcamera) are the python bindings to the libcamera api (in C++). It gives more control of the camera, but is more complicated and can change under developement. python3-picamera2 is built on top of libcamera, is easier to code and more stable.

old: raspivid -t 0 -w 640 -h 480 -fps 120 -o video.h264 -n --denoise cdn_off --ex off --ss 10000
libcamera-vid --level 4.2 --framerate 120 --width 1280 --height 720 --denoise cdn_off -o video.h264
    but h264 slows down fps -> use mjpeg and lower resolutions!
libcamera-vid is yet a bit slower than raspivid (deprecated picamera stack). The former is said to be limited to 60 fps and relying more on CPU, while raspivid can go faster and use more GPU.
V3 raspicam model is fastest, but could be tailored by the 60 fps limit of libcamera-vid stack.
This command captures video continuously at a resolution of 640x480 and a frame rate of 120 FPS without a preview window, while disabling automatic exposure control.

Start a picamera2 streamer for .h264 and rtsp:
    libcamera-vid -t 0 --inline --listen --width 640 --height 480 --codec h264 -n -o tcp://0.0.0.0:8554 (cannot stand a client disconnect)
    Der raw tcp/h264 stream ist etwas pixelig und zeitverzögert, aber nicht so schlimm wie der rtsp-Stream von mediamtx.
    Am flüssigsten läuft das MJPEG streaming in HASS

    Test it by VLC tcp/h264://192.168.178.67:8554/
    or: ffmpeg -i rtsp://192.168.178.67:8554 -t 5 -f null -

mediamtx (https://github.com/bluenviron/mediamtx, user pi) kann direkt auf die picamera2 zugreifen und einen rtsp-Stream absetzen mit folgender 
---- mediamtx.yml:
rtmp: no
hls: no
webrtc: no
srt: no

paths:
  cam:
    source: rpiCamera
----
Der Stream ist unter rtsp://127.0.0.1:8554/cam in HASS bei Integration 'Generic Camera' (nicht mehr über configuration.yaml) zugänglich, aber verzerrt (farbuntief) und zeitverzögert.
Dasselbe schlechte Bild ist mit VLC unter network stream: rtsp://192.168.178.67:8554/cam der Fall.
Damit ist bisher keine gute Methode gefunden, H264 von picamera2 an das lokale HASS zu übertragen.

----python picamera2 using mjpeg encoding on capture:
# compare to https://github.com/raspberrypi/picamera2/blob/main/examples/dual_encode.py
from picamera2 import Picamera2
from picamera2.encoders import JpegEncoder
import time

picam2 = Picamera2()
config = picam2.create_video_configuration(main={"size": (1920, 1080)})
picam2.configure(config)

encoder = JpegEncoder(q=70)  # Adjust quality as needed
picam2.start_recording(encoder, 'output.mjpeg')

time.sleep(10)  # Record for 10 seconds
picam2.stop_recording()

----picamera2 capture as mjpeg to ioBuffer for optimal capture rate (FPS), afterwards transform to h264:
# long recording at high resolution might exhaust memory
import io
from picamera2 import Picamera2
from picamera2.encoders import JpegEncoder
from picamera2.outputs import FileOutput
import time
import av  # for H.264 encoding, python3-av - pythonic bindings for FFmpeg's libraries

class MJPEGBuffer(io.BytesIO):
    def __init__(self):
        super().__init__()
        self.frames = []

    def write(self, buf):
        self.frames.append(buf)
        return len(buf)

def mjpeg_to_h264(mjpeg_frames, output_file, fps=30):
    with av.open(output_file, 'w') as output:
        stream = output.add_stream('h264', rate=fps)
        stream.width = 1920  # Set to your actual width
        stream.height = 1080  # Set to your actual height
        stream.pix_fmt = 'yuv420p'

        for frame_data in mjpeg_frames:
            packet = av.packet.Packet(frame_data)
            for frame in av.codec_context.CodecContext(codec='mjpeg').decode(packet):
                frame = frame.reformat(format='yuv420p')
                for packet in stream.encode(frame):
                    output.mux(packet)

        # Flush the stream
        for packet in stream.encode():
            output.mux(packet)

# Set up PiCamera2
picam2 = Picamera2()
picam2.configure(picam2.create_video_configuration(main={"size": (1920, 1080)}))

# Set up MJPEG buffer
mjpeg_buffer = MJPEGBuffer()

# Start MJPEG encoding
encoder = JpegEncoder()
picam2.start_recording(encoder, FileOutput(mjpeg_buffer))

# Record for a certain duration
record_time = 10  # seconds
time.sleep(record_time)

# Stop recording
picam2.stop_recording()

print(f"Captured {len(mjpeg_buffer.frames)} frames in {record_time} seconds")
print(f"Average FPS: {len(mjpeg_buffer.frames) / record_time}")

# Convert MJPEG to H.264
print("Converting MJPEG to H.264...")
mjpeg_to_h264(mjpeg_buffer.frames, 'output.h264', fps=30)
print("Conversion complete")

---- not tested:
import subprocess
from picamera2 import Picamera2

# Initialize the camera
picam2 = Picamera2()
video_config = picam2.create_video_configuration({"size": (1280, 720)})
picam2.configure(video_config)

# Start the camera
picam2.start()

# Use FFmpeg to create an RTSP stream
ffmpeg_command = [
    'ffmpeg',
    '-f', 'rawvideo',
    '-pix_fmt', 'yuv420p',
    '-s', '1280x720',
    '-i', '-',  # Input from stdin
    '-c:v', 'libx264',
    '-preset', 'ultrafast',
    '-f', 'rtsp',
    'rtsp://0.0.0.0:8554/stream'  # RTSP output URL
]

# Start FFmpeg process
ffmpeg_process = subprocess.Popen(ffmpeg_command, stdin=subprocess.PIPE)

try:
    while True:
        # Capture frames from the camera and write them to FFmpeg's stdin
        frame = picam2.capture_array()
        ffmpeg_process.stdin.write(frame.tobytes())
finally:
    # Stop the camera and FFmpeg process when done
    picam2.stop()
    ffmpeg_process.stdin.close()
    ffmpeg_process.wait()
----
Explanation
Picamera2 Initialization: The script initializes the Picamera2 object and configures it for video capture at a resolution of 1280x720.
FFmpeg Command: FFmpeg is used to encode the video stream into H.264 format and serve it over RTSP. The ffmpeg command is set up to read raw video data from standard input (stdin) and output it as an RTSP stream.
Streaming Loop: In a loop, frames are captured from the camera and written to FFmpeg's standard input, which then streams them via RTSP.
Accessing the Stream
Once the script is running, you can access the RTSP stream using a media player like VLC by opening the network stream at rtsp://<your_pi_ip_address>:8554/stream.
This setup allows you to stream video from your Raspberry Pi camera over RTSP, making it accessible for integration with applications like Home Assistant.

----
----
https://github.com/raspberrypi/picamera2/blob/main/examples/mjpeg_server.py

python and mqtt:
mehr zeitliche Auflösung als 0.5 sek (2 FPS) scheint HASS über mqtt images nicht aufzubringen.
----pub_images.py (D:\rpisteige\mosquitto)
import paho.mqtt.client as mqtt
import time

def on_connect(client, userdata, flags, rc):
    print(f"Connected with result code {rc}")
    # If rc is 0, connection was successful
    if rc == 0:
        print("Successfully connected to MQTT broker")
    else:
        print(f"Failed to connect, return code {rc}")

def on_publish(client, userdata, mid):
    print("Message Published")


images = ["selfie2014.jpg", "britta.jpg", "chris.jpg", "nata.jpg", "stefa.jpg"]
client = mqtt.Client()
client.on_connect = on_connect
client.on_publish = on_publish
client.username_pw_set(username="bird", password="bird24")

# Connect to the broker
client.connect("127.0.0.1", 1883)

# Start the loop
client.loop_start()

# Wait for connection to be established
time.sleep(2)

for x in range(100):
    for i in images:
        with open(i, "rb") as image_file:
            image_data = image_file.read()

        client.publish("/home/camera/image", image_data)
        # client.publish("/home/camera/image", "hallo")

        # Wait for the message to be published
        time.sleep(0.5)

# Stop the loop
client.loop_stop()

----

picamera2/Flask und mjpeg aus dockercam container:
HASS Integration 'MJPEG IP camera' oder 'picture' auf Endpoint /video_feed

----mjpeg_stream.py
from flask import Flask, Response
from picamera2 import Picamera2
import io

app = Flask(__name__)

picam2 = Picamera2()
picam2.configure(picam2.create_video_configuration(main={"size": (640, 480)}))
picam2.start()

'''
def generate_frames():
    while True:
        stream = io.BytesIO()
        picam2.capture_file(stream, format='jpeg')
        yield (b'--frame\r\n'
               b'Content-Type: image/jpeg\r\n\r\n' + stream.getvalue() + b'\r\n')
        time.sleep(0.1)  # Adjust this value to control frame rate
'''
def generate_frames():
    while True:
        stream = io.BytesIO()
        picam2.capture_file(stream, format='jpeg')
        frame = stream.getvalue()
        yield (b'--frame\r\n'
               b'Content-Type: image/jpeg\r\n'
               b'Content-Length: ' + f"{len(frame)}".encode() + b'\r\n'
               b'\r\n' + frame + b'\r\n')

@app.route('/')
def index():
    return """
    <html>
    <body>
    <img src="/video_feed">
    </body>
    </html>
    """

@app.route('/snapshot')
def snapshot():
    stream = io.BytesIO()
    picam2.capture_file(stream, format='jpeg')
    return Response(stream.getvalue(), mimetype='image/jpeg')

# only this endpoint is necessary:
@app.route('/video_feed')
def video_feed():
    return Response(generate_frames(),
                    mimetype='multipart/x-mixed-replace; boundary=frame')

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=8000)
----

Video recording picamera2 /h264 at 40 FPS:
bringt in (1920, 1080) auch nicht mehr als 30 FPS, 40 FPS erst bei (640, 480) => Hardware gibt nicht mehr her
----
from picamera2 import Picamera2
from picamera2.encoders import H264Encoder
import time

picam2 = Picamera2()

video_config = picam2.create_video_configuration(
    main={"size": (1920, 1080)},
    controls={"FrameDurationLimits": (25000, 25000)}
)
# FrameDurationLimits control is set to (25000, 25000), which corresponds to 40 fps (1,000,000 microseconds / 25,000 = 40 fps)
picam2.configure(video_config)

encoder = H264Encoder(bitrate=10000000)  # 10 Mbps

output_file = "output.h264"
picam2.start_recording(encoder, output_file)

# Record for 5 seconds
time.sleep(5)

picam2.stop_recording()
----

Video recording picamera2 /mjpeg at 40 FPS:
Die Leistung ist nicht besser als mit h264: bei (1920, 1080) entstehen 143 Frames/5 sec wie bei h264, also unter 30 FPS
----
from picamera2 import Picamera2
from picamera2.encoders import MJPEGEncoder
from picamera2.outputs import FileOutput
import time

# Initialize the camera
picam2 = Picamera2()

# Configure the camera
config = picam2.create_video_configuration(main={"size": (1920, 1080)}, controls={"FrameRate": 40})
picam2.configure(config)

# Create an MJPEG encoder
encoder = MJPEGEncoder(bitrate=10000000)  # 10 Mbps bitrate

# Set up the output file
output = FileOutput("output.mjpeg")

# Start the camera
picam2.start()

# Start recording
picam2.start_encoder(encoder, output)

# Record for 5 seconds
time.sleep(5)

# Stop recording
picam2.stop_encoder()

# Stop the camera
picam2.stop()

print("Recording complete. Output saved as output.mjpeg")
----

comparing with C programs capture:
    raspivid -w 1920 -h 1080 -fps 40 -t 5000 -o output.mjpeg -cd MJPEG (picamera1, not tested)
    rpicam-vid -t 5000 --codec mjpeg -o output.mjpeg --width 1920 --height 1080 --framerate 40 (not tested)
    libcamera-vid -t 5000 --codec mjpeg -o output.mjpeg --width 1920 --height 1080 --framerate 40 
        gives 106 frames in 5 secs due to reportings
    libcamera-vid -t 5000 --codec mjpeg -o output.mjpeg --width 1920 --height 1080 --framerate 40 --verbose 0 --nopreview --denoise cdn_off (--quality 50 is mjpeg only)  (--intra 0 --inline is h264 only)
            ergibt 111 Frames und ein 10 secs video(?)
    libcamera-vid --codec h264 --width 1920 --height 1080 --framerate 60 -t 5000 -o output.h264 --level 4.2 --intra 0 --denoise cdn_off --nopreview
        surprise: das ergibt 60 FPS! (301 Frames in 5 secs), aber mit stark gezackten Linien...
    libcamera-vid --codec h264 --width 1920 --height 1080 --framerate 60 -t 5000 -o output.h264 --level 4.2 --intra 0 --denoise cdn_off --nopreview --bitrate 25000000 --shutter 8333
        die weiteren von perplexity vorgeschlagenen Optionen verbessern die jagged lines nicht, Resolution (1280x720) oder Quality (mjpeg) müssten geopfert werden.
    libcamera-vid --codec h264 --width 1280 --height 720 --framerate 60 -t 5000 -o output.h264 --level 4.2 --intra 0 --denoise cdn_off --nopreview
        immer noch zackige Linien
    libcamera-vid --codec h264 --width 640 --height 480 --framerate 60 -t 5000 -o output.h264 --level 4.2 --intra 0 --denoise cdn_off --nopreview
        auch hier gezackte Linien trotz niedriger resolution...
    